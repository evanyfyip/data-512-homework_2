{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORES Ranking Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to post to Wikimedias ORES ranking system for each wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, urllib.parse\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading api keys from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can query ORES, we need to extract all the necessary information for each page. In particular we need the revision id. To get this we can query the wikipedia api. Below I have written a function that calls the api given an article title and returns the corresponding revision id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_revid(article_title):\n",
    "    \"\"\"\n",
    "    Extracts the most recent revision id for the wikipedia article\n",
    "    that is specified by the `article_title`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_title : str\n",
    "        The title of the wikipedia article\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": article_title,\n",
    "        \"prop\": \"info\",\n",
    "        \"inprop\": \"url|talkid\"\n",
    "    }\n",
    "\n",
    "    response = session.get(url=URL, params=PARAMS)\n",
    "    response_dict = response.json()\n",
    "\n",
    "    pages = response_dict[\"query\"][\"pages\"]\n",
    "\n",
    "    # Extracting the most recent revision id\n",
    "    for _ , v in pages.items():\n",
    "        return v['lastrevid']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to parse through each line of the `us_cities_by_state_SEPT.2023.csv` file and extract the revision ids for each article. We will store the extracted revision ids into a new csv file under the `data_clean` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22157it [1:36:37,  3.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data_raw/us_cities_by_state_SEPT.2023.csv\")\n",
    "\n",
    "with open(\"./data_clean/us_cities_revid.csv\", \"w+\") as f:\n",
    "    f.write(\"state,page_title,revision_id,url\\n\")\n",
    "    for city in tqdm(df.itertuples()):\n",
    "        revision_id = get_article_revid(city.page_title)\n",
    "        f.write(f'\"{city.state}\",\"{city.page_title}\",\"{revision_id}\",\"{city.url}\"\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the revision ids extracted and stored away, we can now start classifying each article via ORES. Using the starter code that was provided by Dr. McDonald, we have an easy way to call the ORES API. First we define some key parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (60.0/5000.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<{email_address}>, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  : \"\"          # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "#\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the `request_ores_score_per_article` function that Dr. McDonald wrote. We can call this function on each article given its revision id and our own credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "\n",
    "    # Extracting the score from the json response\n",
    "    score = json_response[\"enwiki\"][\"scores\"][str(article_revid)][\"articlequality\"][\"score\"]\n",
    "    return score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start classifying each of the articles we need to load in the csv file that contains the stored revision ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_df = pd.read_csv(\"./data_intermediate/us_cities_revid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19935it [3:36:36,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20997it [3:42:39,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22157it [4:04:01,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_access_token = os.getenv(\"wiki_access_token\")\n",
    "with open(\"./data_clean/us_cities_score.csv\", \"w+\") as f1, open(\"./data_clean/us_cities_score_failures.csv\", \"w+\") as f2:\n",
    "    f1.write(\"state,page_title,revision_id,score,url\\n\")\n",
    "    f2.write(\"state,page_title,revision_id,url\\n\")\n",
    "    for row in tqdm(us_cities_df.itertuples()):\n",
    "        try:\n",
    "            score = request_ores_score_per_article(\n",
    "                article_revid=row.revision_id,\n",
    "                email_address=\"evan@yipsite.net\",\n",
    "                access_token=wiki_access_token)\n",
    "            prediction = score['prediction']\n",
    "            f1.write(f'\"{row.state}\",\"{row.page_title}\",\"{row.revision_id}\",\"{prediction}\",\"{row.url}\"\\n')\n",
    "        except Exception as e:\n",
    "            f2.write(f'\"{row.state}\",\"{row.page_title}\",\"{row.revision_id}\",\"{row.url}\"\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above it seemed like a bunch of the articles failed. Since there were too many for to display in the output, we can analyze this simply by reading in the saved output and comparing the articles in each file to the starting csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18961, 5)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.read_csv(\"../data_intermediate/us_cities_score.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the following cell three times and updating the throttle wait time, we have successfully extracted scores for all of the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading in the failures\n",
    "failures_df = pd.read_csv(\"../data_clean/us_cities_score_failures.csv\")\n",
    "\n",
    "# Running ORES again, this time appending to the scores file, but overwriting the fails\n",
    "wiki_access_token = os.getenv(\"wiki_access_token\")\n",
    "with open(\"./data_clean/us_cities_score.csv\", \"a\") as f1, open(\"./data_clean/us_cities_score_failures.csv\", \"w+\") as f2:\n",
    "    f2.write(\"state,page_title,revision_id,url\\n\")\n",
    "    for row in tqdm(failures_df.itertuples()):\n",
    "        try:\n",
    "            score = request_ores_score_per_article(\n",
    "                article_revid=row.revision_id,\n",
    "                email_address=\"evan@yipsite.net\",\n",
    "                access_token=wiki_access_token)\n",
    "            prediction = score['prediction']\n",
    "            f1.write(f'\"{row.state}\",\"{row.page_title}\",\"{row.revision_id}\",\"{prediction}\",\"{row.url}\"\\n')\n",
    "        except Exception as e:\n",
    "            f2.write(f'\"{row.state}\",\"{row.page_title}\",\"{row.revision_id}\",\"{row.url}\"\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For formatting purposes we will reload in the stored scores and sort them by State and city in alphabetical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18956</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>1169591845</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wamsutter,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18957</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>1176370621</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wheatland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18958</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>1166347917</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Worland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18959</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>1166334449</td>\n",
       "      <td>GA</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wright,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18960</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>1171182284</td>\n",
       "      <td>C</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Yoder,_Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         state          page_title  revision_id score  \\\n",
       "18956  Wyoming  Wamsutter, Wyoming   1169591845    GA   \n",
       "18957  Wyoming  Wheatland, Wyoming   1176370621    GA   \n",
       "18958  Wyoming    Worland, Wyoming   1166347917    GA   \n",
       "18959  Wyoming     Wright, Wyoming   1166334449    GA   \n",
       "18960  Wyoming      Yoder, Wyoming   1171182284     C   \n",
       "\n",
       "                                                    url  \n",
       "18956  https://en.wikipedia.org/wiki/Wamsutter,_Wyoming  \n",
       "18957  https://en.wikipedia.org/wiki/Wheatland,_Wyoming  \n",
       "18958    https://en.wikipedia.org/wiki/Worland,_Wyoming  \n",
       "18959     https://en.wikipedia.org/wiki/Wright,_Wyoming  \n",
       "18960      https://en.wikipedia.org/wiki/Yoder,_Wyoming  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores = pd.read_csv(\"../data_intermediate/us_cities_score.csv\")\n",
    "final_scores = final_scores.sort_values([\"state\", \"page_title\"])\n",
    "final_scores.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe appears to be sorted correctly since Wyoming is at the very bottom and the page titles are also in alphabetical order. The last step is to save the data file back into the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores.to_csv(\"../data_clean/us_cities_score_sorted.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msds_sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
